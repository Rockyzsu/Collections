信息获取度:

没有该特征是的 信息熵

有该特征时的信息熵

差值就是 信息增益

信息增益最大的作为根节点



ID3: information gain

其他方法只是重新定义了其度量方法

C4.5 gain ratio

cart : gini index



连续型的属性需要离散化



深度太大, 分的太细,容易过拟合



优点:

小规模数据集有效

缺点:

连续型数据变量不好

类别较多是,错误增加较快



# 决策树 剪枝

* 限制深度, 叶子节点数,叶子节点的样本数



# 集成算法

* 多种机器学习方法合成在一起,取平均

  



# 随机森林: 

* 多个决策树, 得到的结果取众数. 并行训练一堆树

* 随机, 数据随机采样, 特征也随机采样 60% - 80%
* 树要不一样,保证其泛化能力

![Selection_045](/home/xda/hub/Collections/决策树 剪枝.assets/Selection_045.png)







![Selection_046](/home/xda/hub/Collections/决策树 剪枝.assets/Selection_046.png)



平时用的数的数量100,200个差不多就够了





# Boosting:

从弱学习器开始加强

串行的算法.

先算出一棵树, 根据残差,继续加强

AdaBoots, XgBoost



# Stacking:

堆叠模型, 几个模型,加在一起, 取一个平均





210300102609

